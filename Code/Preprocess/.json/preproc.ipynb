{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J5QX6Bet3jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "315d22ab-ffc8-4b5f-9fc3-9b2ba8d08ec7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YukDN2V7OCs",
        "outputId": "7188a4aa-cb4b-404c-bbb3-92dda0f6b9b8"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import cv2\n",
        "import traceback\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwBeLV9A8pP3",
        "outputId": "cdc84131-3b0f-45fe-da9f-225b3688c9f3"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Bbex1d7eSS"
      },
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "data = []\n",
        "f = open('/content/drive/MyDrive/hateful-memes/train.jsonl','r')\n",
        "for line in f.readlines():\n",
        "    jsonobject = json.loads(line)\n",
        "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
        "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],jsonobject['label'],\n",
        "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
        "    \n",
        "df = pd.DataFrame(data,columns = ['id','text','img',\n",
        "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
        "df.to_csv('/content/drive/MyDrive/hateful-memes/csv/train.csv',index = None)\n",
        "\n",
        "data = []\n",
        "f = open('/content/drive/MyDrive/hateful-memes/dev_unseen.jsonl','r')\n",
        "for line in f.readlines():\n",
        "    jsonobject = json.loads(line)\n",
        "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
        "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],jsonobject['label'],\n",
        "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
        "    \n",
        "df = pd.DataFrame(data,columns = ['id','text','img',\n",
        "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
        "df.to_csv('/content/drive/MyDrive/hateful-memes/csv/dev1.csv',index = None)\n",
        "\n",
        "data = []\n",
        "f = open('/content/drive/MyDrive/hateful-memes/dev_unseen.jsonl','r')\n",
        "for line in f.readlines():\n",
        "    jsonobject = json.loads(line)\n",
        "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
        "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],jsonobject['label'],\n",
        "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
        "    \n",
        "df = pd.DataFrame(data,columns = ['id','text','img',\n",
        "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
        "df.to_csv('/content/drive/MyDrive/hateful-memes/csv/dev2.csv',index = None)\n",
        "\n",
        "data = []\n",
        "f = open('/content/drive/MyDrive/hateful-memes/test_seen.jsonl','r')\n",
        "for line in f.readlines():\n",
        "    jsonobject = json.loads(line)\n",
        "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
        "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],1,\n",
        "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
        "    \n",
        "df = pd.DataFrame(data,columns = ['id','text','img',\n",
        "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
        "df.to_csv('/content/drive/MyDrive/hateful-memes/csv/test1.csv',index = None)\n",
        "\n",
        "data = []\n",
        "f = open('/content/drive/MyDrive/hateful-memes/test.jsonl','r')\n",
        "for line in f.readlines():\n",
        "    jsonobject = json.loads(line)\n",
        "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
        "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],1,\n",
        "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
        "    \n",
        "df = pd.DataFrame(data,columns = ['id','text','img',\n",
        "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
        "df.to_csv('/content/drive/MyDrive/hateful-memes/csv/test2.csv',index = None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgeQWYUE9KOz"
      },
      "source": [
        "img_size = 256\n",
        "def resize_to_square(im):\n",
        "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
        "    ratio = float(img_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    # new_size should be in (width, height) format\n",
        "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
        "    delta_w = img_size - new_size[1]\n",
        "    delta_h = img_size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    color = [0, 0, 0]\n",
        "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
        "    return new_im\n",
        "\n",
        "def load_image(path, name):\n",
        "    image = (Image.open(path + name))\n",
        "    \n",
        "    transform1 = T.Compose([\n",
        "        T.Scale(img_size),\n",
        "        T.CenterCrop((img_size, img_size)),\n",
        "    ])\n",
        "    new_image = transform1(image)\n",
        "    new_image = np.array(new_image)\n",
        "    if len(new_image.shape) == 2:\n",
        "        new_image = np.repeat(new_image.reshape(img_size,img_size,1),3,axis = 2)\n",
        "    \n",
        "    if new_image.shape[2] > 3:\n",
        "        new_image = new_image[:,:,:3]\n",
        "    return np.array(new_image)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw1ckaRX9NiF"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/train.csv')\n",
        "train2 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/dev1.csv')\n",
        "test1 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/test1.csv')\n",
        "train3 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/dev2.csv')\n",
        "test2 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/test2.csv')\n",
        "test = test1.append(test2)\n",
        "train2 = train3.append(train2).drop_duplicates('id',keep='first')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9rxo2ER9ecx",
        "outputId": "7925ccb7-f284-404e-ecd6-5f88c6ff1f1b"
      },
      "source": [
        "id_pic = {}\n",
        "id_label = {}\n",
        "cache = train[['id','text','label']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    id_label[cache[i,0]]  = cache[i,2]\n",
        "    \n",
        "cache = train2[['id','text','label']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    id_label[cache[i,0]]  = cache[i,2]\n",
        "    \n",
        "cache = test[['id','text']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    \n",
        "ids_list = list(id_pic.keys())  \n",
        "isfound = set()\n",
        "features2 = {}\n",
        "features = {}\n",
        "pairs = 0\n",
        "count1 = 0\n",
        "count2 = 0\n",
        "f = open('/content/drive/MyDrive/hateful-memes/hate/hateful/model/text_pairs.csv','w+')\n",
        "for id,v in id_pic.items():\n",
        "    key = v.replace('\"','').replace(\"'\",'').replace(\" \",'')\n",
        "    features[id] = key\n",
        "    for id2,v in features.items():\n",
        "        if id == id2 or str(id) + ' ' + str(id2) in isfound:\n",
        "            continue\n",
        "        if v == key:\n",
        "            print(str(id) + ' ' + str(id2),file = f)\n",
        "            pairs += 1\n",
        "            isfound.add(str(id) + ' ' + str(id2))\n",
        "            isfound.add(str(id2) + ' ' + str(id))\n",
        "        \n",
        "print(count1,count2)\n",
        "print(pairs)\n",
        "print(len(isfound))\n",
        "f.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0\n",
            "3707\n",
            "7414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IsH1oJz-VvW",
        "outputId": "9f45cae3-b3d6-468b-8adc-27be22116ddc"
      },
      "source": [
        "id_pic = {}\n",
        "id_label = {}\n",
        "cache = train[['id','img','label']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    id_label[cache[i,0]]  = cache[i,2]\n",
        "    \n",
        "cache = train2[['id','img','label']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    id_label[cache[i,0]]  = cache[i,2]\n",
        "    \n",
        "cache = test[['id','img']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "ids_list = list(id_pic.keys())        \n",
        "batch_size = 16\n",
        "features = {}\n",
        "features2 = {}\n",
        "n_batches = len(ids_list)//batch_size + 1\n",
        "pairs = 0\n",
        "f = open('/content/drive/MyDrive/hateful-memes/hate/hateful/model/img_pairs.csv','w+')\n",
        "batch_ids = ids_list\n",
        "\n",
        "isfound3 = set()\n",
        "for i,id in enumerate(batch_ids):\n",
        "    if len(batch_ids) == 0:\n",
        "        continue\n",
        "    try:\n",
        "        image = load_image(\"/content/drive/MyDrive/hateful-memes/\", id_pic[id])\n",
        "        features[id] = image[160:188:2,160:188:2,:].astype(int)\n",
        "        if len(set(features[id].sum(axis = 2).reshape(-1).tolist())) <= 1:\n",
        "            continue\n",
        "        for id2,v in features.items():\n",
        "            if id == id2 or str(id) + ' ' + str(id2) in isfound3:\n",
        "                continue\n",
        "            if abs(np.mean(v - features[id])) < 2.5 and np.std(v - features[id]) < 2.5 and np.std(features[id].sum(axis = 2)) > 5:\n",
        "                print(str(id) + ' ' + str(id2),file = f)\n",
        "                isfound3.add(str(id) + ' ' + str(id2))\n",
        "                isfound3.add(str(id2) + ' ' + str(id))\n",
        "                pairs += 1\n",
        "                break\n",
        "                    \n",
        "    except:\n",
        "        print(id,str(traceback.format_exc()))\n",
        "        \n",
        "features = {}\n",
        "print(pairs)\n",
        "for i,id in enumerate(batch_ids):\n",
        "    if len(batch_ids) == 0:\n",
        "        continue\n",
        "    try:\n",
        "        image = load_image(\"/content/drive/MyDrive/hateful-memes/\", id_pic[id])\n",
        "        features[id] = image[175:190:1,150:165:1,:].astype(int)\n",
        "        if len(set(features[id].sum(axis = 2).reshape(-1).tolist())) <= 1:\n",
        "            continue\n",
        "        for id2,v in features.items():\n",
        "            if id == id2 or str(id) + ' ' + str(id2) in isfound3:\n",
        "                continue\n",
        "            if abs(np.mean(v - features[id])) < 2.5 and np.std(v - features[id]) < 2.5 and np.std(features[id].sum(axis = 2)) > 5:\n",
        "                print(str(id) + ' ' + str(id2),file = f)\n",
        "                isfound3.add(str(id) + ' ' + str(id2))\n",
        "                isfound3.add(str(id2) + ' ' + str(id))\n",
        "                pairs += 1\n",
        "                break\n",
        "                    \n",
        "    except:\n",
        "        print(id,str(traceback.format_exc()))        \n",
        "\n",
        "features = {}\n",
        "print(pairs)\n",
        "for i,id in enumerate(batch_ids):\n",
        "    if len(batch_ids) == 0:\n",
        "        continue\n",
        "    try:\n",
        "        image = load_image(\"/content/drive/MyDrive/hateful-memes/\", id_pic[id])\n",
        "        features[id] = image[100:128:2,100:128:2,:].astype(int)\n",
        "        if len(set(features[id].sum(axis = 2).reshape(-1).tolist())) <= 1:\n",
        "            continue\n",
        "        for id2,v in features.items():\n",
        "            if id == id2 or str(id) + ' ' + str(id2) in isfound3:\n",
        "                continue\n",
        "            if abs(np.mean(v - features[id])) < 2.5 and np.std(v - features[id]) < 2.5 and np.std(features[id].sum(axis = 2)) > 5:\n",
        "                print(str(id) + ' ' + str(id2),file = f)\n",
        "                isfound3.add(str(id) + ' ' + str(id2))\n",
        "                isfound3.add(str(id2) + ' ' + str(id))\n",
        "                pairs += 1\n",
        "                break\n",
        "                    \n",
        "    except:\n",
        "        print(id,str(traceback.format_exc())) \n",
        "        \n",
        "features = {}\n",
        "print(pairs)\n",
        "for i,id in enumerate(batch_ids):\n",
        "    if len(batch_ids) == 0:\n",
        "        continue\n",
        "    try:\n",
        "        image = load_image(\"/content/drive/MyDrive/hateful-memes/\", id_pic[id])\n",
        "        features[id] = image[80:100:2,175:200:2,:].astype(int)\n",
        "        if len(set(features[id].sum(axis = 2).reshape(-1).tolist())) <= 1:\n",
        "            continue\n",
        "        for id2,v in features.items():\n",
        "            if id == id2 or str(id) + ' ' + str(id2) in isfound3:\n",
        "                continue\n",
        "            if abs(np.mean(v - features[id])) < 2.5 and np.std(v - features[id]) < 2.5 and np.std(features[id].sum(axis = 2)) > 5:\n",
        "                print(str(id) + ' ' + str(id2),file = f)\n",
        "#                 print(id,id2)\n",
        "                isfound3.add(str(id) + ' ' + str(id2))\n",
        "                isfound3.add(str(id2) + ' ' + str(id))\n",
        "                pairs += 1\n",
        "                break\n",
        "                    \n",
        "    except:\n",
        "        print(id,str(traceback.format_exc()))           \n",
        "                \n",
        "print(pairs)\n",
        "print(len(isfound))\n",
        "f.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1489\n",
            "1973\n",
            "2420\n",
            "2711\n",
            "7414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ovGQ6AmVB1l",
        "outputId": "c1544c5e-fa3f-4944-86a5-b82e7a4f21be"
      },
      "source": [
        "f = open('/content/drive/MyDrive/hateful-memes/hate/hateful/model/same_id.csv','w+')\n",
        "bad_id = set()\n",
        "for pair in isfound3:\n",
        "    if pair in isfound:\n",
        "        array = sorted([int(x) for x in pair.split(\" \")])\n",
        "        if id_label.get(int(array[0]),-1) != id_label.get(int(array[1]),-1) and id_label.get(int(array[0]),-1) != -1 and id_label.get(int(array[1]),-1) != -1:\n",
        "            print(array,id_label.get(int(array[0]),-1),id_label.get(int(array[1]),-1))\n",
        "        else:\n",
        "            print(str(array[0]) + \" \" + str(array[1]), file = f)\n",
        "            bad_id.add(array[1])\n",
        "print(len(bad_id))                \n",
        "f.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRmeUfolVpr5",
        "outputId": "c7300ff2-ee78-471d-b8da-88c49e188e4d"
      },
      "source": [
        "print(array,id_label.get(int(array[0]),-1),id_label.get(int(array[1]),-1))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[35097, 36804] 0 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
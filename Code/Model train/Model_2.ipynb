{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55a3d09d26cb466b9653e95dbb73469a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_045a5b0adf8544e0837baf16ba2adbdd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_50a204cced4c4a5899f8e24ecdeb9b61",
              "IPY_MODEL_126b1854f4dd4f2aa3d822151b46005a"
            ]
          }
        },
        "045a5b0adf8544e0837baf16ba2adbdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50a204cced4c4a5899f8e24ecdeb9b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2c8cb8de06864328b8f0496ca7cd46c1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c6e0b1ff1e7149ac9f695b010a263d14"
          }
        },
        "126b1854f4dd4f2aa3d822151b46005a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3465d761a9f34a62882e4855fb4eba27",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 199MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_84c69ca8db3b4367b026cd3a13cb1182"
          }
        },
        "2c8cb8de06864328b8f0496ca7cd46c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c6e0b1ff1e7149ac9f695b010a263d14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3465d761a9f34a62882e4855fb4eba27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84c69ca8db3b4367b026cd3a13cb1182": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZqqnGkyjTlB",
        "outputId": "21f233e4-4ed9-411c-d890-1b119f751555"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0_8FtoDkK4q"
      },
      "source": [
        "import sklearn\n",
        "import time\n",
        "import datetime,math,os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import lightgbm as lgb\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import traceback\n",
        "from torchvision import transforms as T\n",
        "\n",
        "MODE = 'TEST' # DEV/TEST\n",
        "ISPREDICT = False \n",
        "weight_path = '/content/drive/MyDrive/hateful-memes/weight'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crs5rqQWkbPD",
        "outputId": "e2222660-9280-4cd9-8bce-e0409cf0ebc5"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/train.csv')\n",
        "train2 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/dev1.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/test1.csv')\n",
        "train3 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/dev2.csv')\n",
        "test2 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/test2.csv')\n",
        "train2 = train3.append(train2).drop_duplicates('id',keep='first')\n",
        "train.shape,train2.shape,test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8500, 8), (30, 8), (30, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO101CAqkxIS"
      },
      "source": [
        "def findId(id):\n",
        "    if id in train.id.tolist():\n",
        "        return \"train\"\n",
        "    if id in train2.id.tolist():\n",
        "        return \"train2\"\n",
        "    if id in train3.id.tolist():\n",
        "        return \"train3\"\n",
        "    if id in test.id.tolist():\n",
        "        return \"test\"\n",
        "    if id in test2.id.tolist():\n",
        "        return \"test2\"\n",
        "    return 'unk'\n",
        "\n",
        "same_id_dict = {}\n",
        "for line in open('/content/drive/MyDrive/hateful-memes/hate/hateful/model/same_id.csv','r'):\n",
        "    if line == '':\n",
        "        continue\n",
        "    array = line.strip().split(' ')\n",
        "    same_id_dict[int(array[1])] = int(array[0])\n",
        "\n",
        "for id,v in same_id_dict.items():\n",
        "    temp_id = id\n",
        "    while(True):\n",
        "        if same_id_dict.get(temp_id, -1) != -1:\n",
        "            temp_id = same_id_dict[temp_id]\n",
        "            \n",
        "        else:\n",
        "            break\n",
        "    same_id_dict[id] = temp_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKxFDLpbky_y"
      },
      "source": [
        "coco_dev = pd.read_csv('/content/drive/MyDrive/hateful-memes/hate/hateful/mmf/coco_output2.csv')\n",
        "id_mmf = {}\n",
        "cache = coco_dev[['id','proba']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_mmf[cache[i,0]] = cache[i,1]\n",
        "    \n",
        "coco_dev = pd.read_csv('/content/drive/MyDrive/hateful-memes/hate/hateful/mmf/coco_output.csv')\n",
        "cache = coco_dev[['id','proba']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_mmf[cache[i,0]] = cache[i,1]\n",
        "\n",
        "coco_dev = pd.read_csv('/content/drive/MyDrive/hateful-memes/hate/hateful/mmf/coco_sub.csv')\n",
        "cache = coco_dev[['id','proba']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_mmf[cache[i,0]] = cache[i,1]\n",
        "    \n",
        "coco_dev = pd.read_csv('/content/drive/MyDrive/hateful-memes/hate/hateful/mmf/coco_sub2.csv')\n",
        "cache = coco_dev[['id','proba']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_mmf[cache[i,0]] = cache[i,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dFW4EmhlFmL"
      },
      "source": [
        "id_feat = {}\n",
        "id_weight_nltk = {}\n",
        "cache = train[['id','label','nltk1','nltk2','nltk3','nltk4']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_feat[cache[i,0]]  = cache[i,2:]\n",
        "    if cache[i,1] == 1:\n",
        "        id_weight_nltk[cache[i,0]] = 1 - np.clip(cache[i,4] - cache[i,2],0,0.5)\n",
        "    else:\n",
        "        id_weight_nltk[cache[i,0]] = 1 - np.clip(cache[i,2] - cache[i,4],0,0.5)\n",
        "\n",
        "cache = train2[['id','label','nltk1','nltk2','nltk3','nltk4']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_feat[cache[i,0]]  = cache[i,2:]\n",
        "    if cache[i,1] == 1:\n",
        "        id_weight_nltk[cache[i,0]] = 1 - np.clip(cache[i,4] - cache[i,2],0,0.5)\n",
        "    else:\n",
        "        id_weight_nltk[cache[i,0]] = 1 - np.clip(cache[i,2] - cache[i,4],0,0.5)\n",
        "\n",
        "        \n",
        "cache = test[['id','label','nltk1','nltk2','nltk3','nltk4']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_feat[cache[i,0]]  = cache[i,2:]\n",
        "    \n",
        "cache = test2[['id','label','nltk1','nltk2','nltk3','nltk4']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_feat[cache[i,0]]  = cache[i,2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Z9np0qlGo7"
      },
      "source": [
        "from tqdm import tqdm, tqdm_notebook\n",
        "img_size = 128\n",
        "def resize_to_square(im):\n",
        "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
        "    ratio = float(img_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    # new_size should be in (width, height) format\n",
        "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
        "    delta_w = img_size - new_size[1]\n",
        "    delta_h = img_size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    color = [0, 0, 0]\n",
        "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
        "    return new_im\n",
        "\n",
        "def load_image(path, name):\n",
        "    image = (Image.open(path + name))\n",
        "    \n",
        "    transform1 = T.Compose([\n",
        "        T.Scale(img_size),\n",
        "        T.CenterCrop((img_size, img_size)),\n",
        "    ])\n",
        "    new_image = transform1(image)\n",
        "    new_image = np.array(new_image)\n",
        "    if len(new_image.shape) == 2:\n",
        "        new_image = np.repeat(new_image.reshape(img_size,img_size,1),3,axis = 2)\n",
        "    \n",
        "    if new_image.shape[2] > 3:\n",
        "        new_image = new_image[:,:,:3]\n",
        "    return new_image/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_vpXLUUlKLy",
        "outputId": "e3e46be6-b4b9-488b-8622-103e0111d43e"
      },
      "source": [
        "\n",
        "pic_cache = {}\n",
        "id_pic = {}\n",
        "id_text = {}\n",
        "id_label = {}\n",
        "cache = train[['id','img','label','text']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    id_label[cache[i,0]] = cache[i,2]\n",
        "    pic_cache[cache[i,0]] = load_image(\"/content/drive/MyDrive/hateful-memes/\", cache[i,1])\n",
        "    id_text[cache[i,0]]  = cache[i,3]\n",
        "\n",
        "\n",
        "id_label2 = {}\n",
        "id_label3 = {}\n",
        "cache = train2[['id','img','label','text']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    if MODE == 'TEST':\n",
        "        id_label[cache[i,0]] = cache[i,2]\n",
        "    else:\n",
        "        id_label2[cache[i,0]] = cache[i,2]\n",
        "    pic_cache[cache[i,0]] = load_image(\"/content/drive/MyDrive/hateful-memes/\", cache[i,1])\n",
        "    id_text[cache[i,0]]  = cache[i,3]\n",
        "\n",
        "ids_list = list(id_label.keys()) \n",
        "\n",
        "cache = test[['id','img','label','text']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    pic_cache[cache[i,0]] = load_image(\"/content/drive/MyDrive/hateful-memes/\", cache[i,1])\n",
        "    id_text[cache[i,0]]  = cache[i,3]\n",
        "    \n",
        "cache = test2[['id','img','label','text']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    pic_cache[cache[i,0]] = load_image(\"/content/drive/MyDrive/hateful-memes/\", cache[i,1])\n",
        "    id_text[cache[i,0]]  = cache[i,3]\n",
        "    if MODE == 'TEST':\n",
        "        id_label2[cache[i,0]] = random.choice([0,1])\n",
        "ids_list2 = list(id_label2.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMB7vUCCzrt9",
        "outputId": "34b7ad90-85ee-47a1-be43-046a4e434afb"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.8.1rc2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnBd67htztnT",
        "outputId": "84c8d5fb-391e-4358-9f67-7cd02b3bb78b"
      },
      "source": [
        "!pip install transformers==3.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.1.95)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.0.45)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E6VADmTlVRk"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                        level = logging.INFO)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tokenizers\n",
        "from transformers import RobertaModel, RobertaConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "            vocab_file='/content/drive/MyDrive/hateful-memes/hate/hateful/roberta-base-vocab.json', \n",
        "            merges_file='/content/drive/MyDrive/hateful-memes/hate/hateful/roberta-base-merges.txt', \n",
        "            lowercase=True,\n",
        "            add_prefix_space=True)\n",
        "\n",
        "max_seq_length = 64\n",
        "\n",
        "def get_input_data(text):\n",
        "    text = \" \" + \" \".join(text.lower().split())\n",
        "    encoding = tokenizer.encode(text)\n",
        "    ids = [0] + encoding.ids + [2]\n",
        "    offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
        "                \n",
        "    pad_len = max_seq_length - len(ids)\n",
        "    if pad_len > 0:\n",
        "        ids += [1] * pad_len\n",
        "        offsets += [(0, 0)] * pad_len\n",
        "    elif pad_len < 0:\n",
        "        ids = ids[:max_seq_length]\n",
        "        offsets = offsets[:max_seq_length]\n",
        "    ids = torch.tensor(ids)\n",
        "    masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
        "        \n",
        "    return ids, masks\n",
        "\n",
        "class DensNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        preloaded = torchvision.models.resnet50(pretrained=True)\n",
        "        hidden1_size = 96\n",
        "        self.features = preloaded\n",
        "        self.features.conv1 = nn.Conv2d(3, 64, 7, 2, 3)\n",
        "        self.fci = nn.Linear(1000, hidden1_size, bias=True)\n",
        "        self.fct = nn.Linear(768 + 4, hidden1_size, bias=True)\n",
        "        self.fc2 = nn.Linear(hidden1_size * 3, 1, bias=True)\n",
        "        self.dropout = nn.Dropout(0.05)\n",
        "#         del preloaded\n",
        "        \n",
        "        self.fc3 = nn.Linear(hidden1_size * 4, 32, bias=True)\n",
        "        self.fc4 = nn.Linear(32, 2, bias=True)\n",
        "        \n",
        "        config = RobertaConfig.from_pretrained(\n",
        "            '/content/drive/MyDrive/hateful-memes/hate/hateful/roberta-base-config.json', output_hidden_states=True)    \n",
        "        self.roberta = RobertaModel.from_pretrained(\n",
        "            '/content/drive/MyDrive/hateful-memes/hate/hateful/roberta-base-pytorch_model.bin', config=config)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fctext = nn.Linear(config.hidden_size, 16)\n",
        "        nn.init.normal_(self.fctext.weight, std=0.02)\n",
        "        nn.init.normal_(self.fctext.bias, 0)\n",
        "\n",
        "    def middle(self, x):\n",
        "        features = self.features(x[0])\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = self.dropout(out)\n",
        "        out = (self.fci(out))\n",
        "        \n",
        "        hs1, hs0, hs = self.roberta(x[1], x[2])\n",
        "        hs0 = torch.cat([hs0,x[3]],1)\n",
        "        hs0 = self.dropout(hs0)\n",
        "        hs0 = (self.fct(hs0))\n",
        "        return [out,hs0]       \n",
        "        \n",
        "    def forward(self, x):\n",
        "        out,hs0 = self.middle(x)\n",
        "        out = (torch.cat([out - hs0,hs0 * out,hs0], 1))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "    def pair_forward(self, x1, x2):\n",
        "        result = []\n",
        "        for x in [x1,x2]:\n",
        "            out,hs0 = self.middle(x)\n",
        "            result.append(torch.cat([out,hs0],1))\n",
        "            \n",
        "        out = (self.fc3(torch.cat([result[0] - result[1],result[0] * result[1]], 1)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc4(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGgLSB14lp-F",
        "outputId": "fc841088-3af3-406c-ad93-f763b0b4d436"
      },
      "source": [
        "img_pairs = []\n",
        "text_pairs = []\n",
        "for line in open('/content/drive/MyDrive/hateful-memes/hate/hateful/model/img_pairs.csv','r'):\n",
        "    if line == '':\n",
        "        continue\n",
        "    array = line.strip().split(' ')\n",
        "    img_pairs.append([int(array[0]),int(array[1])])\n",
        "\n",
        "            \n",
        "    \n",
        "for line in open('/content/drive/MyDrive/hateful-memes/hate/hateful/model/text_pairs.csv','r'):\n",
        "    if line == '':\n",
        "        continue\n",
        "    array = line.strip().split(' ')\n",
        "    text_pairs.append([int(array[0]),int(array[1])])\n",
        "\n",
        "ids_list3 = [x for x in ids_list2]\n",
        "print(len(img_pairs),len(text_pairs),len(ids_list3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2711 3707 640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqCkuFODlzvr"
      },
      "source": [
        "id_pair = []\n",
        "id_sample_weight = {}\n",
        "for pairs in [text_pairs,img_pairs]:\n",
        "    id2cluster = {}\n",
        "    cluster2id = {}\n",
        "    clusterid = 0 \n",
        "    for line in pairs:\n",
        "        id0 = line[0]\n",
        "        id1 = line[1]\n",
        "        if id_label.get(id0,-1) == -1 and id_label2.get(id0,-1) == -1:\n",
        "            continue\n",
        "        if id_label.get(id1,-1) == -1 and id_label2.get(id1,-1) == -1:\n",
        "            continue\n",
        "        if id_label2.get(id0,-1) != -1 or id_label2.get(id1,-1) != -1:\n",
        "            if same_id_dict.get(id0,id0) != same_id_dict.get(id1,id1):\n",
        "                id_pair.append(line)\n",
        "    \n",
        "        if id2cluster.get(id0,-1) == -1 and id2cluster.get(id1,-1) == -1:\n",
        "            cluster2id[clusterid] = set()\n",
        "            cluster2id[clusterid].add(id0)\n",
        "            cluster2id[clusterid].add(id1)\n",
        "            id2cluster[id0] = clusterid\n",
        "            id2cluster[id1] = clusterid\n",
        "            clusterid += 1\n",
        "        elif id2cluster.get(id0,-1) != -1 or id2cluster.get(id1,-1) != -1:\n",
        "            if id2cluster.get(id0,-1) != -1:\n",
        "                clusterid_temp = id2cluster[id0]\n",
        "                cluster2id[clusterid_temp].add(id1)\n",
        "                id2cluster[id1] = clusterid_temp\n",
        "            if id2cluster.get(id1,-1) != -1:\n",
        "                clusterid_temp = id2cluster[id1]\n",
        "                cluster2id[clusterid_temp].add(id0)\n",
        "                id2cluster[id0] = clusterid_temp\n",
        "\n",
        "    clusterinfo = {}\n",
        "    valid_y = []\n",
        "    pred_y = []\n",
        "    for k,v in cluster2id.items():\n",
        "        l = list(v)\n",
        "        info = [0,0,0]\n",
        "        for id in l:\n",
        "            if id_label.get(id,-1) == -1:\n",
        "                info[2] +=1\n",
        "            if id_label.get(id,-1) != -1:\n",
        "                info[id_label.get(id,-1)] += 1\n",
        "\n",
        "                \n",
        "        distinct_id = set()\n",
        "        for id in l:\n",
        "            temp_id = id\n",
        "            while(True):\n",
        "                if same_id_dict.get(temp_id, -1) != -1:\n",
        "                    temp_id = same_id_dict[temp_id]\n",
        "                else:\n",
        "                    break\n",
        "            distinct_id.add(temp_id)\n",
        "        \n",
        "        for id in l:\n",
        "            id_sample_weight[id] = 1/len(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715,
          "referenced_widgets": [
            "55a3d09d26cb466b9653e95dbb73469a",
            "045a5b0adf8544e0837baf16ba2adbdd",
            "50a204cced4c4a5899f8e24ecdeb9b61",
            "126b1854f4dd4f2aa3d822151b46005a",
            "2c8cb8de06864328b8f0496ca7cd46c1",
            "c6e0b1ff1e7149ac9f695b010a263d14",
            "3465d761a9f34a62882e4855fb4eba27",
            "84c69ca8db3b4367b026cd3a13cb1182"
          ]
        },
        "id": "KaXkekcZl41c",
        "outputId": "0e8dc41d-7c12-43cf-98cf-930735355655"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "criterion2 = nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "pair_batch_size = 8\n",
        "valid_batch_num = 16\n",
        "n_batches = len(ids_list)//batch_size + 1\n",
        "valid_batch_num = len(ids_list3)//batch_size + 1\n",
        "valid_pair_batch_num = len(id_pair)//pair_batch_size + 1\n",
        "id_pred_all = []\n",
        "id_pred2_all = []\n",
        "\n",
        "for fold in range(3):\n",
        "    model = DensNet()\n",
        "    model.to('cuda')\n",
        "    model2 = DensNet()\n",
        "    model2.to('cuda')\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-5, betas=(0.9, 0.999))\n",
        "    optimizer2 = torch.optim.AdamW(model2.parameters(), lr=1.5e-5, betas=(0.9, 0.999))\n",
        "    if ISPREDICT:\n",
        "        model.load_state_dict(torch.load(weight_path + '/model2_' + str(fold) + '.pt'))\n",
        "        model2.load_state_dict(torch.load(weight_path + '/model_2_' + str(fold) + '.pt'))\n",
        "    else:\n",
        "        for ep in range(6):\n",
        "            print('ep',ep)\n",
        "            ids_list_train = ids_list.copy()\n",
        "            random.shuffle(ids_list_train)\n",
        "            random.shuffle(img_pairs)\n",
        "            random.shuffle(text_pairs)\n",
        "            model.train()\n",
        "            model2.train()\n",
        "            tloss = 0.0\n",
        "            tloss2 = 0.0\n",
        "            tloss3 = 0.0\n",
        "            for b in range(n_batches):\n",
        "                pairs_backward = False\n",
        "                for pairs in [img_pairs,text_pairs]:\n",
        "                    start = b*pair_batch_size\n",
        "                    end = (b+1)*pair_batch_size\n",
        "                    if start >= len(pairs):\n",
        "                        continue\n",
        "                    batch_ids = pairs[start:end]\n",
        "                    batch_images_x1 = []\n",
        "                    batch_images_x2 = []\n",
        "                    batch_text_x = []\n",
        "                    batch_text2_x = []\n",
        "                    batch_text_x2 = []\n",
        "                    batch_text2_x2 = []\n",
        "                    batch_feat_x1 = []\n",
        "                    batch_feat_x2 = []\n",
        "                    y = []\n",
        "                    for i,id in enumerate(batch_ids):\n",
        "                        if len(batch_ids) == 0:\n",
        "                            continue\n",
        "                        try:\n",
        "                            if random.random() > 0.5:\n",
        "                                id1 = id[0]\n",
        "                                id2 = id[1]\n",
        "                            else:\n",
        "                                id1 = id[1]\n",
        "                                id2 = id[0]\n",
        "                            batch_images_x1.append(pic_cache[id1])\n",
        "                            batch_images_x2.append(pic_cache[id2])\n",
        "                            batch_feat_x1.append(id_feat[id1])\n",
        "                            batch_feat_x2.append(id_feat[id2])\n",
        "                            text_feat = get_input_data(id_text[id1])\n",
        "                            batch_text_x.append(text_feat[0])\n",
        "                            batch_text2_x.append(text_feat[1])\n",
        "                            text_feat = get_input_data(id_text[id2])\n",
        "                            batch_text_x2.append(text_feat[0])\n",
        "                            batch_text2_x2.append(text_feat[1])                    \n",
        "                            y.append([id_label.get(id1,-1),id_label.get(id2,-1)])\n",
        "\n",
        "                        except:\n",
        "                            print(id,str(traceback.format_exc()))\n",
        "                    y = torch.FloatTensor(y).to('cuda')\n",
        "                    batch_images_x1 = torch.FloatTensor(batch_images_x1).to('cuda')\n",
        "                    batch_images_x2 = torch.FloatTensor(batch_images_x2).to('cuda')\n",
        "                    batch_feat_x1 = torch.FloatTensor(batch_feat_x1).to('cuda')\n",
        "                    batch_feat_x2 = torch.FloatTensor(batch_feat_x2).to('cuda')\n",
        "                    batch_text_x = torch.stack(batch_text_x, 0).to('cuda')\n",
        "                    batch_text2_x = torch.stack(batch_text2_x, 0).to('cuda')\n",
        "                    batch_text_x2 = torch.stack(batch_text_x2, 0).to('cuda')\n",
        "                    batch_text2_x2 = torch.stack(batch_text2_x2, 0).to('cuda')\n",
        "                    output = model2.pair_forward(x1 = [batch_images_x1.permute(0,3,1,2),batch_text_x,batch_text2_x,batch_feat_x1],\n",
        "                                        x2 = [batch_images_x2.permute(0,3,1,2),batch_text_x2,batch_text2_x2,batch_feat_x2])\n",
        "                    output2 = F.sigmoid(output)\n",
        "                    loss = criterion2(output.view(-1), y.view(-1))\n",
        "                    loss = loss * (1 - (y.view(-1) == -1).float()) * 0.15\n",
        "                    loss2 = F.relu(0.3 - (output2[:,0] - output2[:,1]) * \n",
        "                                   (y[:,0] - y[:,1])) * abs(y[:,0] - y[:,1]) * (1 - (y[:,0] == -1).float()) * (1 - (y[:,1] == -1).float())\n",
        "                    loss3 = loss.mean() + loss2.mean()\n",
        "                    loss3.backward()\n",
        "                    pairs_backward = True\n",
        "                    tloss2 += loss.mean().item()\n",
        "                    tloss3 += loss2.mean().item()\n",
        "\n",
        "                # normal train\n",
        "                start = b*batch_size\n",
        "                end = (b+1)*batch_size\n",
        "                if start >= len(ids_list_train):\n",
        "                    continue\n",
        "                batch_ids = ids_list_train[start:end]\n",
        "                batch_images = []\n",
        "                batch_feat = []\n",
        "                batch_text = []\n",
        "                batch_text2 = []\n",
        "                sample_weight = []\n",
        "                y = []\n",
        "                for i,id in enumerate(batch_ids):\n",
        "                    if len(batch_ids) == 0:\n",
        "                        continue\n",
        "                    try:\n",
        "                        batch_images.append(pic_cache[id])\n",
        "                        batch_feat.append(id_feat[id])\n",
        "                        y.append([id_label[id]])\n",
        "                        sample_weight.append([max(0.2,id_sample_weight.get(id,1) ** 0.8)])\n",
        "                        text_feat = get_input_data(id_text[id])\n",
        "                        batch_text.append(text_feat[0])\n",
        "                        batch_text2.append(text_feat[1])\n",
        "                    except:\n",
        "                        print(id,str(traceback.format_exc()))\n",
        "\n",
        "                y = torch.FloatTensor(y).to('cuda')\n",
        "                batch_feat = torch.FloatTensor(batch_feat).to('cuda')\n",
        "                batch_images = torch.FloatTensor(batch_images).to('cuda')\n",
        "                batch_text = torch.stack(batch_text, 0).to('cuda')\n",
        "                batch_text2 = torch.stack(batch_text2, 0).to('cuda')\n",
        "                sample_weight = torch.FloatTensor(sample_weight).to('cuda')\n",
        "                output = model([batch_images.permute(0,3,1,2),batch_text,batch_text2,batch_feat])\n",
        "                loss = (criterion2((output), y) * sample_weight).mean()\n",
        "                loss.backward()\n",
        "                if b % 3 == 0:\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                if b % 4 == 0 and pairs_backward:\n",
        "                    optimizer2.step()\n",
        "                    optimizer2.zero_grad()\n",
        "                tloss += loss.item() \n",
        "\n",
        "            print(\"loss:\",tloss/len(ids_list),tloss2/len(ids_list),tloss3/len(ids_list))\n",
        "    if True:\n",
        "        model.eval()\n",
        "        model2.eval()\n",
        "\n",
        "        id_pred = {}\n",
        "        for b in range(valid_batch_num):\n",
        "            start = b*batch_size\n",
        "            end = (b+1)*batch_size\n",
        "            if start >= len(ids_list3):\n",
        "                continue\n",
        "            batch_ids = ids_list3[start:end]\n",
        "            batch_images = []\n",
        "            batch_feat = []\n",
        "            batch_text = []\n",
        "            batch_text2 = []\n",
        "            y = []\n",
        "            for i,id in enumerate(batch_ids):\n",
        "                if len(batch_ids) == 0:\n",
        "                    continue\n",
        "                try:\n",
        "                    batch_images.append(pic_cache[id])\n",
        "                    y.append(id_label2[id])\n",
        "                    batch_feat.append(id_feat[id])\n",
        "                    text_feat = get_input_data(id_text[id])\n",
        "                    batch_text.append(text_feat[0])\n",
        "                    batch_text2.append(text_feat[1])\n",
        "                except:\n",
        "                    print(id,str(traceback.format_exc()))\n",
        "\n",
        "            batch_images = torch.FloatTensor(batch_images).to('cuda')\n",
        "            batch_feat = torch.FloatTensor(batch_feat).to('cuda')\n",
        "            batch_text = torch.stack(batch_text, 0).to('cuda')\n",
        "            batch_text2 = torch.stack(batch_text2, 0).to('cuda')\n",
        "            output = model([batch_images.permute(0,3,1,2),batch_text,batch_text2,batch_feat]).view(-1)\n",
        "            valid_y2 = torch.FloatTensor(y).to('cuda')\n",
        "            output2 = F.sigmoid(output)\n",
        "            temp = F.sigmoid(output).tolist()\n",
        "            for i,id in enumerate(batch_ids):\n",
        "                id_pred[id] = temp[i]\n",
        " \n",
        "        \n",
        "        id_pred2 = {}\n",
        "        for b in range(valid_pair_batch_num):\n",
        "            start = b*pair_batch_size\n",
        "            end = (b+1)*pair_batch_size\n",
        "            if start >= len(id_pair):\n",
        "                continue\n",
        "            batch_ids = id_pair[start:end]\n",
        "            batch_images_x1 = []\n",
        "            batch_images_x2 = []\n",
        "            batch_text_x = []\n",
        "            batch_text2_x = []\n",
        "            batch_text_x2 = []\n",
        "            batch_text2_x2 = []\n",
        "            batch_feat_x1 = []\n",
        "            batch_feat_x2 = []\n",
        "            y = []\n",
        "            for i,id in enumerate(batch_ids):\n",
        "                if len(batch_ids) == 0:\n",
        "                    continue\n",
        "                try:\n",
        "                    id1 = id[0]\n",
        "                    id2 = id[1]\n",
        "                    batch_images_x1.append(pic_cache[id1])\n",
        "                    batch_images_x2.append(pic_cache[id2])\n",
        "                    batch_feat_x1.append(id_feat[id1])\n",
        "                    batch_feat_x2.append(id_feat[id2])\n",
        "                    text_feat = get_input_data(id_text[id1])\n",
        "                    batch_text_x.append(text_feat[0])\n",
        "                    batch_text2_x.append(text_feat[1])\n",
        "                    text_feat = get_input_data(id_text[id2])\n",
        "                    batch_text_x2.append(text_feat[0])\n",
        "                    batch_text2_x2.append(text_feat[1])                    \n",
        "                    y.append([id_label2.get(id1,-1),id_label2.get(id2,-1)])\n",
        "\n",
        "                except:\n",
        "                    print(id,str(traceback.format_exc()))\n",
        "            y = torch.FloatTensor(y).to('cuda')\n",
        "            batch_images_x1 = torch.FloatTensor(batch_images_x1).to('cuda')\n",
        "            batch_images_x2 = torch.FloatTensor(batch_images_x2).to('cuda')\n",
        "            batch_feat_x1 = torch.FloatTensor(batch_feat_x1).to('cuda')\n",
        "            batch_feat_x2 = torch.FloatTensor(batch_feat_x2).to('cuda')\n",
        "            batch_text_x = torch.stack(batch_text_x, 0).to('cuda')\n",
        "            batch_text2_x = torch.stack(batch_text2_x, 0).to('cuda')\n",
        "            batch_text_x2 = torch.stack(batch_text_x2, 0).to('cuda')\n",
        "            batch_text2_x2 = torch.stack(batch_text2_x2, 0).to('cuda')\n",
        "            output = model2.pair_forward(x1 = [batch_images_x1.permute(0,3,1,2),batch_text_x,batch_text2_x,batch_feat_x1],\n",
        "                                    x2 = [batch_images_x2.permute(0,3,1,2),batch_text_x2,batch_text2_x2,batch_feat_x2])\n",
        "            output2 = F.sigmoid(output)\n",
        "            temp = F.sigmoid(output).tolist()\n",
        "            for i,id in enumerate(batch_ids):\n",
        "                id1 = id[0]\n",
        "                id2 = id[1]\n",
        "                id_pred2[id1] = id_pred2.get(id1,[])\n",
        "                id_pred2[id2] = id_pred2.get(id2,[])\n",
        "                id_pred2[id1].append(temp[i][0])\n",
        "                id_pred2[id2].append(temp[i][1])\n",
        "\n",
        "        if MODE == \"VALID\":\n",
        "            valid_y = []\n",
        "            pred_y = []\n",
        "            for id in ids_list3:\n",
        "                pred_y.append(id_pred[id])\n",
        "                valid_y.append(id_label2[id])\n",
        "            print(sklearn.metrics.roc_auc_score(valid_y,pred_y),\n",
        "                  sklearn.metrics.accuracy_score(valid_y,np.array(pred_y)>0.5))\n",
        "\n",
        "            valid_y2 = []\n",
        "            pred_y2 = []\n",
        "            for id,prob in id_pred2.items():\n",
        "                if id_label2.get(id,-1) == -1:\n",
        "                    continue\n",
        "                pred_y2.append(sum(id_pred2[id])/len(id_pred2[id]))\n",
        "                valid_y2.append(id_label2[id])\n",
        "            print(sklearn.metrics.roc_auc_score(valid_y2,pred_y2), \n",
        "                  sklearn.metrics.accuracy_score(valid_y2,np.array(pred_y2)>0.5))\n",
        "        \n",
        "    if not ISPREDICT:\n",
        "        torch.save(model.state_dict(), weight_path + '/model2_' + str(fold) + '.pt')\n",
        "        torch.save(model2.state_dict(), weight_path + '/model_2_' + str(fold) + '.pt')                \n",
        "    id_pred_all.append(id_pred)\n",
        "    id_pred2_all.append(id_pred2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55a3d09d26cb466b9653e95dbb73469a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ep 0\n",
            "loss: 0.02714096417497186 0.008726320572635707 0.010248441446134272\n",
            "ep 1\n",
            "loss: 0.021253033609074705 0.0071307165333453345 0.006388455814079327\n",
            "ep 2\n",
            "loss: 0.01823714791150654 0.006216303036493414 0.004387330065974418\n",
            "ep 3\n",
            "loss: 0.014647099595736054 0.00547686628328965 0.0031418699646259055\n",
            "ep 4\n",
            "loss: 0.010726571185185628 0.004651273830410313 0.002120005269855884\n",
            "ep 5\n",
            "loss: 0.007942844941335566 0.003972990703275975 0.001416356786428129\n",
            "ep 0\n",
            "loss: 0.02790998915714376 0.009021471724790685 0.010557605888256256\n",
            "ep 1\n",
            "loss: 0.021984247665194905 0.007696364840602174 0.007767664488843259\n",
            "ep 2\n",
            "loss: 0.018883921223528246 0.006762536394683754 0.005100693184663268\n",
            "ep 3\n",
            "loss: 0.015267811068717172 0.005976293456685894 0.003337414301274454\n",
            "ep 4\n",
            "loss: 0.011160065455471768 0.005314438435742083 0.0024034079462289812\n",
            "ep 5\n",
            "loss: 0.008365190864047583 0.004529075317632626 0.0014300356969675597\n",
            "ep 0\n",
            "loss: 0.027074453806175906 0.008685102013542372 0.010520498536527157\n",
            "ep 1\n",
            "loss: 0.021231780883143928 0.007631247511681388 0.008171660525724291\n",
            "ep 2\n",
            "loss: 0.018658189142451567 0.006649594178094583 0.005217109142638305\n",
            "ep 3\n",
            "loss: 0.015584475446273298 0.0058313209773863065 0.0035558135743965122\n",
            "ep 4\n",
            "loss: 0.012859355577651192 0.004931555504746297 0.0022471034340560436\n",
            "ep 5\n",
            "loss: 0.00936495182553635 0.004228246910716681 0.0014955193466123413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwuUVHxgnJ2t"
      },
      "source": [
        "id_pred_2 = {}\n",
        "for id in ids_list3:\n",
        "    id_pred_2[id] = (id_pred_all[0][id] +  id_pred_all[1][id] + id_pred_all[2][id])/3\n",
        "            \n",
        "id_pred2_2 = {}\n",
        "for id,prob in id_pred2_all[0].items():\n",
        "    if id_label2.get(id,-1) == -1:\n",
        "        continue\n",
        "    id_pred2_2[id] = np.mean([np.mean(id_pred2_all[0][id]),np.mean(id_pred2_all[1][id]),np.mean(id_pred2_all[2][id])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m29CZ-MonQ0E",
        "outputId": "111ddb92-0780-4ffb-d1be-9874b2aa33ea"
      },
      "source": [
        "id_pred2_1_np = [[],[],[],[]]\n",
        "for id,prob in id_pred_all[0].items():\n",
        "    id_pred2_1_np[0].append(id_pred_all[0][id])\n",
        "    id_pred2_1_np[1].append(id_pred_all[1][id])\n",
        "    id_pred2_1_np[2].append(id_pred_all[2][id])\n",
        "    id_pred2_1_np[3].append(id_mmf.get(id,0))\n",
        "print(np.corrcoef(np.stack(id_pred2_1_np)))\n",
        "\n",
        "id_pred2_2_np = [[],[],[],[]]\n",
        "for id,prob in id_pred2_all[0].items():\n",
        "    if id_label2.get(id,-1) == -1:\n",
        "        continue\n",
        "    id_pred2_2_np[0].append(np.mean(id_pred2_all[0][id]))\n",
        "    id_pred2_2_np[1].append(np.mean(id_pred2_all[1][id]))\n",
        "    id_pred2_2_np[2].append(np.mean(id_pred2_all[2][id]))\n",
        "    id_pred2_2_np[3].append(id_mmf.get(id,0))\n",
        "print(np.corrcoef(np.stack(id_pred2_2_np)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.63109247 0.64341354 0.30990265]\n",
            " [0.63109247 1.         0.61065198 0.2809076 ]\n",
            " [0.64341354 0.61065198 1.         0.31915296]\n",
            " [0.30990265 0.2809076  0.31915296 1.        ]]\n",
            "[[1.         0.51739734 0.56532276 0.29756999]\n",
            " [0.51739734 1.         0.56810912 0.24012563]\n",
            " [0.56532276 0.56810912 1.         0.32734346]\n",
            " [0.29756999 0.24012563 0.32734346 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLtSwLt0nWTD"
      },
      "source": [
        "import pickle\n",
        "f = open(\"/content/drive/MyDrive/hateful-memes/weight/submission_2.pkl\", 'wb')\n",
        "pickle.dump([id_pred_2,id_pred2_2,id_mmf], f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
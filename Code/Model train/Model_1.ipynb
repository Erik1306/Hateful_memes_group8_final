{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "42a4a04299434f319624e1de84bbd980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d4610a1b4d3646ae89dca0792c99ec7e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f23b9a9f119e44e49afcd90ae4c5e2e3",
              "IPY_MODEL_f41fe3b434614878896e3c020cc752bf"
            ]
          }
        },
        "d4610a1b4d3646ae89dca0792c99ec7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f23b9a9f119e44e49afcd90ae4c5e2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2ea2d1fb01fb46d18d13ba5916d6fd97",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5fd4704cca18462491f5732207f2df13"
          }
        },
        "f41fe3b434614878896e3c020cc752bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3c61ddd5d3be4c3886cf17c9161727fc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:02&lt;00:00, 45.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_529347e3a2f842eca5ee7d1558f6671c"
          }
        },
        "2ea2d1fb01fb46d18d13ba5916d6fd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5fd4704cca18462491f5732207f2df13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c61ddd5d3be4c3886cf17c9161727fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "529347e3a2f842eca5ee7d1558f6671c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcVmLGGHaQKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545c2c19-f1fe-438f-dbc5-318a004e99a3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1kq95-ZTT2-"
      },
      "source": [
        "import sklearn\n",
        "import time\n",
        "import datetime,math,os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import lightgbm as lgb\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import traceback\n",
        "from torchvision import transforms as T\n",
        "\n",
        "MODE = 'TEST' # DEV/TEST\n",
        "ISPREDICT = False  \n",
        "weight_path = '/content/drive/MyDrive/hateful-memes/weight'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6-HQ0l5amtj",
        "outputId": "871d00c0-84e3-48d9-cabf-7bd2a60b20a5"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/hateful-memes/hate/hateful/input2/train_p1.csv')\n",
        "train2 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/dev1.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/test1.csv')\n",
        "train3 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/dev2.csv')\n",
        "test2 = pd.read_csv('/content/drive/MyDrive/hateful-memes/csv/test2.csv')\n",
        "train2 = train3.append(train2).drop_duplicates('id',keep='first')\n",
        "train.shape,train2.shape,test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8500, 8), (30, 8), (30, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGDdwQiYcCNF"
      },
      "source": [
        "\n",
        "def findId(id):\n",
        "    if id in train.id.tolist():\n",
        "        return \"train\"\n",
        "    if id in train2.id.tolist():\n",
        "        return \"train2\"\n",
        "    if id in train3.id.tolist():\n",
        "        return \"train3\"\n",
        "    if id in test.id.tolist():\n",
        "        return \"test\"\n",
        "    if id in test2.id.tolist():\n",
        "        return \"test2\"\n",
        "    return 'unk'\n",
        "\n",
        "same_id_dict = {}\n",
        "for line in open('/content/drive/MyDrive/hateful-memes/hate/hateful/model/same_id.csv','r'):\n",
        "    if line == '':\n",
        "        continue\n",
        "    array = line.strip().split(' ')\n",
        "    same_id_dict[int(array[1])] = int(array[0])\n",
        "\n",
        "for id,v in same_id_dict.items():\n",
        "    temp_id = id\n",
        "    while(True):\n",
        "        if same_id_dict.get(temp_id, -1) != -1:\n",
        "            temp_id = same_id_dict[temp_id]\n",
        "            \n",
        "        else:\n",
        "            break\n",
        "    same_id_dict[id] = temp_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlQ510y2cEOa"
      },
      "source": [
        "coco_dev = pd.read_csv('/content/drive/MyDrive/hateful-memes/hate/hateful/mmf/coco_output2.csv')\n",
        "id_mmf = {}\n",
        "cache = coco_dev[['id','proba']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_mmf[cache[i,0]] = cache[i,1]\n",
        "    \n",
        "coco_dev = pd.read_csv('/content/drive/MyDrive/hateful-memes/hate/hateful/mmf/coco_output.csv')\n",
        "cache = coco_dev[['id','proba']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_mmf[cache[i,0]] = cache[i,1]\n",
        "\n",
        "coco_dev = pd.read_csv('/content/drive/MyDrive/hateful-memes/hate/hateful/mmf/coco_sub.csv')\n",
        "cache = coco_dev[['id','proba']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_mmf[cache[i,0]] = cache[i,1]\n",
        "    \n",
        "coco_dev = pd.read_csv('/content/drive/MyDrive/hateful-memes/hate/hateful/mmf/coco_sub2.csv')\n",
        "cache = coco_dev[['id','proba']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_mmf[cache[i,0]] = cache[i,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYtmIGlvcVIs"
      },
      "source": [
        "id_feat = {}\n",
        "id_weight_nltk = {}\n",
        "cache = train[['id','label','nltk1','nltk2','nltk3','nltk4']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_feat[cache[i,0]]  = cache[i,2:]\n",
        "    if cache[i,1] == 1:\n",
        "        id_weight_nltk[cache[i,0]] = 1 - np.clip(cache[i,4] - cache[i,2],0,0.5)\n",
        "    else:\n",
        "        id_weight_nltk[cache[i,0]] = 1 - np.clip(cache[i,2] - cache[i,4],0,0.5)\n",
        "\n",
        "cache = train2[['id','label','nltk1','nltk2','nltk3','nltk4']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_feat[cache[i,0]]  = cache[i,2:]\n",
        "    if cache[i,1] == 1:\n",
        "        id_weight_nltk[cache[i,0]] = 1 - np.clip(cache[i,4] - cache[i,2],0,0.5)\n",
        "    else:\n",
        "        id_weight_nltk[cache[i,0]] = 1 - np.clip(cache[i,2] - cache[i,4],0,0.5)\n",
        "\n",
        "        \n",
        "cache = test[['id','label','nltk1','nltk2','nltk3','nltk4']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_feat[cache[i,0]]  = cache[i,2:]\n",
        "    \n",
        "cache = test2[['id','label','nltk1','nltk2','nltk3','nltk4']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_feat[cache[i,0]]  = cache[i,2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYAVgO1ScXvb"
      },
      "source": [
        "from tqdm import tqdm, tqdm_notebook\n",
        "img_size = 128\n",
        "def resize_to_square(im):\n",
        "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
        "    ratio = float(img_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    # new_size should be in (width, height) format\n",
        "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
        "    delta_w = img_size - new_size[1]\n",
        "    delta_h = img_size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    color = [0, 0, 0]\n",
        "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
        "    return new_im\n",
        "\n",
        "def load_image(path, name):\n",
        "    image = (Image.open(path + name))\n",
        "    \n",
        "    transform1 = T.Compose([\n",
        "        T.Scale(img_size),\n",
        "        T.CenterCrop((img_size, img_size)),\n",
        "    ])\n",
        "    new_image = transform1(image)\n",
        "    new_image = np.array(new_image)\n",
        "    if len(new_image.shape) == 2:\n",
        "        new_image = np.repeat(new_image.reshape(img_size,img_size,1),3,axis = 2)\n",
        "    \n",
        "    if new_image.shape[2] > 3:\n",
        "        new_image = new_image[:,:,:3]\n",
        "    return new_image/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LhRn57ScbC6",
        "outputId": "ceaf7450-7b3e-436a-a219-3f0fdb01d625"
      },
      "source": [
        "pic_cache = {}\n",
        "id_pic = {}\n",
        "id_text = {}\n",
        "id_label = {}\n",
        "cache = train[['id','img','label','text']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    id_label[cache[i,0]] = cache[i,2]\n",
        "    pic_cache[cache[i,0]] = load_image(\"/content/drive/MyDrive/hateful-memes/\", cache[i,1])\n",
        "    id_text[cache[i,0]]  = cache[i,3]\n",
        "\n",
        "\n",
        "id_label2 = {}\n",
        "id_label3 = {}\n",
        "cache = train2[['id','img','label','text']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    if MODE == 'TEST':\n",
        "        id_label[cache[i,0]] = cache[i,2]\n",
        "    else:\n",
        "        id_label2[cache[i,0]] = cache[i,2]\n",
        "    pic_cache[cache[i,0]] = load_image(\"/content/drive/MyDrive/hateful-memes/\", cache[i,1])\n",
        "    id_text[cache[i,0]]  = cache[i,3]\n",
        "\n",
        "ids_list = list(id_label.keys()) \n",
        "\n",
        "cache = test[['id','img','label','text']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    pic_cache[cache[i,0]] = load_image(\"/content/drive/MyDrive/hateful-memes/\", cache[i,1])\n",
        "    id_text[cache[i,0]]  = cache[i,3]\n",
        "    \n",
        "cache = test2[['id','img','label','text']].values\n",
        "for i in range(cache.shape[0]):\n",
        "    id_pic[cache[i,0]]  = cache[i,1]\n",
        "    pic_cache[cache[i,0]] = load_image(\"/content/drive/MyDrive/hateful-memes/\", cache[i,1])\n",
        "    id_text[cache[i,0]]  = cache[i,3]\n",
        "    if MODE == 'TEST':\n",
        "        id_label2[cache[i,0]] = random.choice([0,1])\n",
        "ids_list2 = list(id_label2.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odVRT3NuzH_z",
        "outputId": "b6e45187-0edd-4ff6-a466-8b7bf9223b3f"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 7.4MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka4DD2cdzn0r",
        "outputId": "e5793ae3-2472-46a7-a171-001ab6970fce"
      },
      "source": [
        "!pip install transformers==3.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 8.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/26/c02ba92ecb8b780bdae4a862d351433c2912fe49469dac7f87a5c85ccca6/tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 12.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (1.19.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (20.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0) (2.4.7)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "  Found existing installation: tokenizers 0.10.2\n",
            "    Uninstalling tokenizers-0.10.2:\n",
            "      Successfully uninstalled tokenizers-0.10.2\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.8.1rc2 transformers-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slwGp_uIcpLK"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                        level = logging.INFO)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tokenizers\n",
        "from transformers import RobertaModel, RobertaConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "            vocab_file='/content/drive/MyDrive/hateful-memes/hate/hateful/roberta-base-vocab.json', \n",
        "            merges_file='/content/drive/MyDrive/hateful-memes/hate/hateful/roberta-base-merges.txt', \n",
        "            lowercase=True,\n",
        "            add_prefix_space=True)\n",
        "\n",
        "max_seq_length = 96\n",
        "\n",
        "def get_input_data(text):\n",
        "    text = \" \" + \" \".join(text.lower().split())\n",
        "    encoding = tokenizer.encode(text)\n",
        "    ids = [0] + encoding.ids + [2]\n",
        "    offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
        "                \n",
        "    pad_len = max_seq_length - len(ids)\n",
        "    if pad_len > 0:\n",
        "        ids += [1] * pad_len\n",
        "        offsets += [(0, 0)] * pad_len\n",
        "    elif pad_len < 0:\n",
        "        ids = ids[:max_seq_length]\n",
        "        offsets = offsets[:max_seq_length]\n",
        "    ids = torch.tensor(ids)\n",
        "    masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
        "        \n",
        "    return ids, masks\n",
        "\n",
        "class DensNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        preloaded = torchvision.models.resnet50(pretrained=True)\n",
        "        hidden1_size = 64\n",
        "        self.features = preloaded\n",
        "        self.features.conv1 = nn.Conv2d(3, 64, 7, 2, 3)\n",
        "        self.fci = nn.Linear(1000, hidden1_size, bias=True)\n",
        "        self.fct = nn.Linear(768 + 4, hidden1_size, bias=True)\n",
        "        self.fc2 = nn.Linear(hidden1_size, 1, bias=True)\n",
        "        \n",
        "        self.fc3 = nn.Linear(hidden1_size * 4, 32, bias=True)\n",
        "        self.fc4 = nn.Linear(32, 2, bias=True)\n",
        "        \n",
        "        config = RobertaConfig.from_pretrained(\n",
        "            '/content/drive/MyDrive/hateful-memes/hate/hateful/roberta-base-config.json', output_hidden_states=True)    \n",
        "        self.roberta = RobertaModel.from_pretrained(\n",
        "            '/content/drive/MyDrive/hateful-memes/hate/hateful/roberta-base-pytorch_model.bin', config=config)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fctext = nn.Linear(config.hidden_size, 16)\n",
        "        nn.init.normal_(self.fctext.weight, std=0.02)\n",
        "        nn.init.normal_(self.fctext.bias, 0)\n",
        "        \n",
        "        hidden_dim = 64\n",
        "        self.gate = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "        self.tabular_dense = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.text_dense = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn_gate = nn.BatchNorm1d(hidden_dim)\n",
        "        \n",
        "    def middle(self, x):\n",
        "        features = self.features(x[0])\n",
        "        out = F.relu(features, inplace=True)\n",
        "#         out = F.adaptive_max_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
        "        out = self.dropout(out)\n",
        "        out = (self.fci(out))\n",
        "        \n",
        "        hs1, hs0, hs = self.roberta(x[1], x[2])\n",
        "        hs0 = torch.cat([hs0,x[3]],1)\n",
        "        hs0 = self.dropout(hs0)\n",
        "        hs0 = (self.fct(hs0))\n",
        "        return [out,hs0]       \n",
        "        \n",
        "    def forward(self, x):\n",
        "        out,hs0 = self.middle(x)\n",
        "        tabular_txt_concat = torch.cat([out, hs0], dim=1)\n",
        "        tab_proba = torch.sigmoid(self.bn_gate(self.gate(tabular_txt_concat)))\n",
        "        out = self.tabular_dense(out).mul(tab_proba) + self.text_dense(hs0).mul(\n",
        "            1 - tab_proba\n",
        "        )\n",
        "        \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "    def pair_forward(self, x1, x2):\n",
        "        result = []\n",
        "        for x in [x1,x2]:\n",
        "            out,hs0 = self.middle(x)\n",
        "            result.append(torch.cat([out,hs0],1))\n",
        "            \n",
        "        out = (self.fc3(torch.cat([result[0] - result[1],result[0] * result[1]], 1)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc4(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSCaR8w3dNSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d55c1d9b-e2c9-4400-9e46-8e45260f165e"
      },
      "source": [
        "img_pairs = []\n",
        "text_pairs = []\n",
        "for line in open('/content/drive/MyDrive/hateful-memes/hate/hateful/model/img_pairs.csv','r'):\n",
        "    if line == '':\n",
        "        continue\n",
        "    array = line.strip().split(' ')\n",
        "    img_pairs.append([int(array[0]),int(array[1])])\n",
        "\n",
        "            \n",
        "    \n",
        "for line in open('/content/drive/MyDrive/hateful-memes/hate/hateful/model/text_pairs.csv','r'):\n",
        "    if line == '':\n",
        "        continue\n",
        "    array = line.strip().split(' ')\n",
        "    text_pairs.append([int(array[0]),int(array[1])])\n",
        "\n",
        "ids_list3 = [x for x in ids_list2]\n",
        "print(len(img_pairs),len(text_pairs),len(ids_list3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2711 3707 640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ABZRL2Ydc7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac52cd51-e091-4761-c10a-41695954dafc"
      },
      "source": [
        "id_bias_test1 = {}\n",
        "for pairs in [text_pairs,img_pairs]:\n",
        "    id2cluster = {}\n",
        "    cluster2id = {}\n",
        "    clusterid = 0 \n",
        "    for line in pairs:\n",
        "        id0 = line[0]\n",
        "        id1 = line[1]\n",
        "        \n",
        "        if id_label.get(id0,-1) == -1 and id0 not in test.id.tolist() :\n",
        "            continue\n",
        "        if id_label.get(id1,-1) == -1 and id1 not in test.id.tolist() :\n",
        "            continue\n",
        "            \n",
        "        if id2cluster.get(id0,-1) == -1 and id2cluster.get(id1,-1) == -1:\n",
        "            cluster2id[clusterid] = set()\n",
        "            cluster2id[clusterid].add(id0)\n",
        "            cluster2id[clusterid].add(id1)\n",
        "            id2cluster[id0] = clusterid\n",
        "            id2cluster[id1] = clusterid\n",
        "            clusterid += 1\n",
        "        elif id2cluster.get(id0,-1) != -1 or id2cluster.get(id1,-1) != -1:\n",
        "            if id2cluster.get(id0,-1) != -1:\n",
        "                clusterid_temp = id2cluster[id0]\n",
        "                cluster2id[clusterid_temp].add(id1)\n",
        "                id2cluster[id1] = clusterid_temp\n",
        "            if id2cluster.get(id1,-1) != -1:\n",
        "                clusterid_temp = id2cluster[id1]\n",
        "                cluster2id[clusterid_temp].add(id0)\n",
        "                id2cluster[id0] = clusterid_temp\n",
        "\n",
        "    clusterinfo = {}\n",
        "    for k,v in cluster2id.items():\n",
        "        l = list(v)\n",
        "        info = [0,0,0]\n",
        "        for id in l:\n",
        "            if id_label.get(id,-1) == -1:\n",
        "                info[2] +=1\n",
        "            if id_label.get(id,-1) != -1:\n",
        "                info[id_label.get(id,-1)] += 1\n",
        "\n",
        "        distinct_id = set()\n",
        "        for id in l:\n",
        "            temp_id = id\n",
        "            while(True):\n",
        "                if same_id_dict.get(temp_id, -1) != -1:\n",
        "                    temp_id = same_id_dict[temp_id]\n",
        "                else:\n",
        "                    break\n",
        "            distinct_id.add(temp_id)\n",
        "        \n",
        "        for id in l:\n",
        "            info2 = info.copy()\n",
        "            if id_label.get(id,-1) == -1:\n",
        "                info2[2] -= 1\n",
        "            if id_label.get(id,-1) != -1:\n",
        "                info2[id_label.get(id,-1)] -= 1\n",
        "            if len(distinct_id) == 2 and info2[0] + info2[1] == 1 and id in test.id.tolist():\n",
        "                id_bias_test1[same_id_dict.get(id,id)] = info2[0] \n",
        "\n",
        "        clusterinfo[k] = info\n",
        "len(id_bias_test1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLR3oDyrdh57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6977f00-2e8d-4174-d9b4-0381b7e61354"
      },
      "source": [
        "id_bias = {}\n",
        "id_pair = []\n",
        "id_sample_weight = {}\n",
        "for pairs in [text_pairs,img_pairs]:\n",
        "    id2cluster = {}\n",
        "    cluster2id = {}\n",
        "    clusterid = 0 \n",
        "    for line in pairs:\n",
        "        id0 = line[0]\n",
        "        id1 = line[1]\n",
        "        if id_label.get(id0,-1) == -1 and id_label2.get(id0,-1) == -1:\n",
        "            continue\n",
        "        if id_label.get(id1,-1) == -1 and id_label2.get(id1,-1) == -1:\n",
        "            continue\n",
        "        if id_label2.get(id0,-1) != -1 or id_label2.get(id1,-1) != -1:\n",
        "            if same_id_dict.get(id0,id0) != same_id_dict.get(id1,id1):\n",
        "                id_pair.append(line)\n",
        "    \n",
        "        if id2cluster.get(id0,-1) == -1 and id2cluster.get(id1,-1) == -1:\n",
        "            cluster2id[clusterid] = set()\n",
        "            cluster2id[clusterid].add(id0)\n",
        "            cluster2id[clusterid].add(id1)\n",
        "            id2cluster[id0] = clusterid\n",
        "            id2cluster[id1] = clusterid\n",
        "            clusterid += 1\n",
        "        elif id2cluster.get(id0,-1) != -1 or id2cluster.get(id1,-1) != -1:\n",
        "            if id2cluster.get(id0,-1) != -1:\n",
        "                clusterid_temp = id2cluster[id0]\n",
        "                cluster2id[clusterid_temp].add(id1)\n",
        "                id2cluster[id1] = clusterid_temp\n",
        "            if id2cluster.get(id1,-1) != -1:\n",
        "                clusterid_temp = id2cluster[id1]\n",
        "                cluster2id[clusterid_temp].add(id0)\n",
        "                id2cluster[id0] = clusterid_temp\n",
        "\n",
        "    clusterinfo = {}\n",
        "    valid_y = []\n",
        "    pred_y = []\n",
        "    for k,v in cluster2id.items():\n",
        "        l = list(v)\n",
        "        info = [0,0,0]\n",
        "        for id in l:\n",
        "            if id_label.get(id,-1) == -1:\n",
        "                info[2] +=1\n",
        "            if id_label.get(id,-1) != -1:\n",
        "                info[id_label.get(id,-1)] += 1\n",
        "\n",
        "                \n",
        "        distinct_id = set()\n",
        "        for id in l:\n",
        "            temp_id = id\n",
        "            while(True):\n",
        "                if same_id_dict.get(temp_id, -1) != -1:\n",
        "                    temp_id = same_id_dict[temp_id]\n",
        "                else:\n",
        "                    break\n",
        "            distinct_id.add(temp_id)\n",
        "        \n",
        "        for id in l:\n",
        "            id_sample_weight[id] = 1/len(l)\n",
        "            info2 = info.copy()\n",
        "            if id_label.get(id,-1) == -1:\n",
        "                info2[2] -= 1\n",
        "            if id_label.get(id,-1) != -1:\n",
        "                info2[id_label.get(id,-1)] -= 1\n",
        "            if len(distinct_id) == 2 and info2[0] + info2[1] == 1 and id_label2.get(id,-1) != -1:\n",
        "                id_bias[id] = info2[0] \n",
        "\n",
        "        clusterinfo[k] = info\n",
        "len(id_bias),len(id_pair)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(114, 522)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylwBMg7Odkai"
      },
      "source": [
        "import pickle\n",
        "f = open(\"/content/drive/MyDrive/hateful-memes/hate/hateful/model/id_bias.pkl\", 'wb')\n",
        "pickle.dump([id_bias,id_bias_test1], f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCj6tyJ7dunz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483cfea0-c391-494d-f787-e90374c4b6af"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "criterion2 = nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "\n",
        "batch_size = 16\n",
        "pair_batch_size = 8\n",
        "valid_batch_num = 16\n",
        "n_batches = len(ids_list)//batch_size + 1\n",
        "valid_batch_num = len(ids_list3)//batch_size + 1\n",
        "\n",
        "id_pred_all = []\n",
        "for fold in range(3):\n",
        "    model = DensNet()\n",
        "    model.to('cuda')\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
        "    if ISPREDICT:\n",
        "        model.load_state_dict(torch.load(weight_path + '/model1_' + str(fold) + '.pt'))\n",
        "    else:\n",
        "        for ep in range(5):\n",
        "            print('ep',ep)\n",
        "            random.shuffle(ids_list)\n",
        "            random.shuffle(img_pairs)\n",
        "            random.shuffle(text_pairs)\n",
        "            model.train()\n",
        "            tloss = 0.0\n",
        "            for b in range(n_batches):\n",
        "\n",
        "                # normal train\n",
        "                start = b*batch_size\n",
        "                end = (b+1)*batch_size\n",
        "                batch_ids = ids_list[start:end]\n",
        "                batch_images = []\n",
        "                batch_feat = []\n",
        "                batch_text = []\n",
        "                batch_text2 = []\n",
        "                sample_weight = []\n",
        "                y = []\n",
        "                for i,id in enumerate(batch_ids):\n",
        "                    if len(batch_ids) == 0:\n",
        "                        continue\n",
        "                    try:\n",
        "                        batch_images.append(pic_cache[id])\n",
        "                        batch_feat.append(id_feat[id])\n",
        "                        y.append([id_label[id]])\n",
        "                        sample_weight.append([(max(id_sample_weight.get(id,1),0.2)) * 0.7 + id_weight_nltk.get(id,1) * 0.3])\n",
        "                        text_feat = get_input_data(id_text[id])\n",
        "                        batch_text.append(text_feat[0])\n",
        "                        batch_text2.append(text_feat[1])\n",
        "                    except:\n",
        "                        print(id,str(traceback.format_exc()))\n",
        "\n",
        "                y = torch.FloatTensor(y).to('cuda')\n",
        "                batch_feat = torch.FloatTensor(batch_feat).to('cuda')\n",
        "                batch_images = torch.FloatTensor(batch_images).to('cuda')\n",
        "                batch_text = torch.stack(batch_text, 0).to('cuda')\n",
        "                batch_text2 = torch.stack(batch_text2, 0).to('cuda')\n",
        "                sample_weight = torch.FloatTensor(sample_weight).to('cuda')\n",
        "                output = model([batch_images.permute(0,3,1,2),batch_text,batch_text2,batch_feat])\n",
        "                loss = (criterion2((output), y) * sample_weight).mean()\n",
        "                loss.backward()\n",
        "                if b % 2 == 0:\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                tloss += loss.item() \n",
        "\n",
        "            print(\"loss:\",tloss/len(ids_list))\n",
        "                  \n",
        "    if True:    \n",
        "        model.eval()\n",
        "        id_pred = {}\n",
        "        for b in range(valid_batch_num):\n",
        "            start = b*batch_size\n",
        "            end = (b+1)*batch_size\n",
        "            if start >= len(ids_list3):\n",
        "                continue\n",
        "            batch_ids = ids_list3[start:end]\n",
        "            batch_images = []\n",
        "            batch_feat = []\n",
        "            batch_text = []\n",
        "            batch_text2 = []\n",
        "            y = []\n",
        "            for i,id in enumerate(batch_ids):\n",
        "                if len(batch_ids) == 0:\n",
        "                    continue\n",
        "                try:\n",
        "                    batch_images.append(pic_cache[id])\n",
        "                    y.append(id_label2[id])\n",
        "                    batch_feat.append(id_feat[id])\n",
        "                    text_feat = get_input_data(id_text[id])\n",
        "                    batch_text.append(text_feat[0])\n",
        "                    batch_text2.append(text_feat[1])\n",
        "                except:\n",
        "                    print(id,str(traceback.format_exc()))\n",
        "\n",
        "            batch_images = torch.FloatTensor(batch_images).to('cuda')\n",
        "            batch_feat = torch.FloatTensor(batch_feat).to('cuda')\n",
        "            batch_text = torch.stack(batch_text, 0).to('cuda')\n",
        "            batch_text2 = torch.stack(batch_text2, 0).to('cuda')\n",
        "            output = model([batch_images.permute(0,3,1,2),batch_text,batch_text2,batch_feat]).view(-1)\n",
        "            valid_y2 = torch.FloatTensor(y).to('cuda')\n",
        "            output2 = F.sigmoid(output)\n",
        "            temp = F.sigmoid(output).tolist()\n",
        "            for i,id in enumerate(batch_ids):\n",
        "                id_pred[id] = temp[i]\n",
        "        \n",
        "        if MODE == \"VALID\":\n",
        "            pred_y = []\n",
        "            valid_y = []\n",
        "            for id in ids_list3:\n",
        "                pred_y.append(id_pred[id])\n",
        "                valid_y.append(id_label2[id])\n",
        "            print(sklearn.metrics.roc_auc_score(valid_y,pred_y),\n",
        "                  sklearn.metrics.accuracy_score(valid_y,np.array(pred_y)>0.5),\n",
        "                  sklearn.metrics.log_loss(valid_y,np.array(pred_y)))\n",
        "\n",
        "\n",
        "    if not ISPREDICT:\n",
        "        torch.save(model.state_dict(), weight_path + '/model1_' + str(fold) + '.pt')       \n",
        "    id_pred_all.append(id_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep 0\n",
            "loss: 0.03030289199773003\n",
            "ep 1\n",
            "loss: 0.026203023260130603\n",
            "ep 2\n",
            "loss: 0.023975765741923276\n",
            "ep 3\n",
            "loss: 0.022025532024748187\n",
            "ep 4\n",
            "loss: 0.019453315282569213\n",
            "ep 0\n",
            "loss: 0.02981033432483673\n",
            "ep 1\n",
            "loss: 0.025306948021930807\n",
            "ep 2\n",
            "loss: 0.023205256269258612\n",
            "ep 3\n",
            "loss: 0.021188356937731013\n",
            "ep 4\n",
            "loss: 0.018885047881042255\n",
            "ep 0\n",
            "loss: 0.03018198686136919\n",
            "ep 1\n",
            "loss: 0.02588056232122814\n",
            "ep 2\n",
            "loss: 0.023680447171716128\n",
            "ep 3\n",
            "loss: 0.021746831951772464\n",
            "ep 4\n",
            "loss: 0.019480546381543666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9nhbzVtd_zU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695,
          "referenced_widgets": [
            "42a4a04299434f319624e1de84bbd980",
            "d4610a1b4d3646ae89dca0792c99ec7e",
            "f23b9a9f119e44e49afcd90ae4c5e2e3",
            "f41fe3b434614878896e3c020cc752bf",
            "2ea2d1fb01fb46d18d13ba5916d6fd97",
            "5fd4704cca18462491f5732207f2df13",
            "3c61ddd5d3be4c3886cf17c9161727fc",
            "529347e3a2f842eca5ee7d1558f6671c"
          ]
        },
        "outputId": "c8370059-d119-4a2c-999c-47ab9dc594ed"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "criterion2 = nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "\n",
        "batch_size = 16\n",
        "pair_batch_size = 4\n",
        "valid_batch_num = 16\n",
        "n_batches = len(ids_list)//batch_size + 1\n",
        "valid_batch_num = len(ids_list3)//batch_size + 1\n",
        "valid_pair_batch_num = len(id_pair)//pair_batch_size + 1\n",
        "\n",
        "\n",
        "id_pred2_all = []\n",
        "for fold in range(3):\n",
        "    model = DensNet()\n",
        "    model.to('cuda')\n",
        "    optimizer2 = torch.optim.AdamW(model.parameters(), lr=1.5e-5, betas=(0.9, 0.999))\n",
        "    if ISPREDICT:\n",
        "        model.load_state_dict(torch.load(weight_path + '/model_1_' + str(fold) + '.pt'))\n",
        "    else:\n",
        "        for ep in range(6):\n",
        "            print('ep',ep)\n",
        "            random.shuffle(img_pairs)\n",
        "            random.shuffle(text_pairs)\n",
        "            model.train()\n",
        "            tloss2 = 0.0\n",
        "            tloss3 = 0.0\n",
        "            for b in range(n_batches):\n",
        "                pairs_backward = False\n",
        "                for pairs in [img_pairs,text_pairs]:\n",
        "                    start = b*pair_batch_size\n",
        "                    end = (b+1)*pair_batch_size\n",
        "                    if start >= len(pairs):\n",
        "                        continue\n",
        "                    batch_ids = pairs[start:end]\n",
        "                    batch_images_x1 = []\n",
        "                    batch_images_x2 = []\n",
        "                    batch_text_x = []\n",
        "                    batch_text2_x = []\n",
        "                    batch_text_x2 = []\n",
        "                    batch_text2_x2 = []\n",
        "                    batch_feat_x1 = []\n",
        "                    batch_feat_x2 = []\n",
        "                    y = []\n",
        "                    for i,id in enumerate(batch_ids):\n",
        "                        if len(batch_ids) == 0:\n",
        "                            continue\n",
        "                        try:\n",
        "                            if random.random() > 0.5:\n",
        "                                id1 = id[0]\n",
        "                                id2 = id[1]\n",
        "                            else:\n",
        "                                id1 = id[1]\n",
        "                                id2 = id[0]\n",
        "                            batch_images_x1.append(pic_cache[id1])\n",
        "                            batch_images_x2.append(pic_cache[id2])\n",
        "                            batch_feat_x1.append(id_feat[id1])\n",
        "                            batch_feat_x2.append(id_feat[id2])\n",
        "                            text_feat = get_input_data(id_text[id1])\n",
        "                            batch_text_x.append(text_feat[0])\n",
        "                            batch_text2_x.append(text_feat[1])\n",
        "                            text_feat = get_input_data(id_text[id2])\n",
        "                            batch_text_x2.append(text_feat[0])\n",
        "                            batch_text2_x2.append(text_feat[1])                    \n",
        "                            y.append([id_label.get(id1,-1),id_label.get(id2,-1)])\n",
        "\n",
        "                        except:\n",
        "                            print(id,str(traceback.format_exc()))\n",
        "                    y = torch.FloatTensor(y).to('cuda')\n",
        "                    batch_images_x1 = torch.FloatTensor(batch_images_x1).to('cuda')\n",
        "                    batch_images_x2 = torch.FloatTensor(batch_images_x2).to('cuda')\n",
        "                    batch_feat_x1 = torch.FloatTensor(batch_feat_x1).to('cuda')\n",
        "                    batch_feat_x2 = torch.FloatTensor(batch_feat_x2).to('cuda')\n",
        "                    batch_text_x = torch.stack(batch_text_x, 0).to('cuda')\n",
        "                    batch_text2_x = torch.stack(batch_text2_x, 0).to('cuda')\n",
        "                    batch_text_x2 = torch.stack(batch_text_x2, 0).to('cuda')\n",
        "                    batch_text2_x2 = torch.stack(batch_text2_x2, 0).to('cuda')\n",
        "                    output = model.pair_forward(x1 = [batch_images_x1.permute(0,3,1,2),batch_text_x,batch_text2_x,batch_feat_x1],\n",
        "                                        x2 = [batch_images_x2.permute(0,3,1,2),batch_text_x2,batch_text2_x2,batch_feat_x2])\n",
        "                    output2 = F.sigmoid(output)\n",
        "                    loss = criterion2(output.view(-1), y.view(-1))\n",
        "                    loss = loss * (1 - (y.view(-1) == -1).float()) * 0.3\n",
        "                    loss2 = F.relu(0.3 - (output2[:,0] - output2[:,1]) * \n",
        "                                   (y[:,0] - y[:,1])) * abs(y[:,0] - y[:,1]) * (1 - (y[:,0] == -1).float()) * (1 - (y[:,1] == -1).float())\n",
        "                    loss3 = loss.mean() + loss2.mean()\n",
        "                    loss3.backward()\n",
        "                    pairs_backward = True\n",
        "                    tloss2 += loss.mean().item()\n",
        "                    tloss3 += loss2.mean().item()\n",
        "                if b % 3 == 0 and pairs_backward:\n",
        "                    optimizer2.step()\n",
        "                    optimizer2.zero_grad()\n",
        "\n",
        "            print(\"loss:\",tloss2/len(ids_list),tloss3/len(ids_list))\n",
        "    if True:\n",
        "        model.eval()\n",
        "\n",
        "        id_pred2 = {}\n",
        "        for b in range(valid_pair_batch_num):\n",
        "            start = b*pair_batch_size\n",
        "            end = (b+1)*pair_batch_size\n",
        "            if start >= len(id_pair):\n",
        "                continue\n",
        "            batch_ids = id_pair[start:end]\n",
        "            batch_images_x1 = []\n",
        "            batch_images_x2 = []\n",
        "            batch_text_x = []\n",
        "            batch_text2_x = []\n",
        "            batch_text_x2 = []\n",
        "            batch_text2_x2 = []\n",
        "            batch_feat_x1 = []\n",
        "            batch_feat_x2 = []\n",
        "            y = []\n",
        "            for i,id in enumerate(batch_ids):\n",
        "                if len(batch_ids) == 0:\n",
        "                    continue\n",
        "                try:\n",
        "                    id1 = id[0]\n",
        "                    id2 = id[1]\n",
        "                    batch_images_x1.append(pic_cache[id1])\n",
        "                    batch_images_x2.append(pic_cache[id2])\n",
        "                    batch_feat_x1.append(id_feat[id1])\n",
        "                    batch_feat_x2.append(id_feat[id2])\n",
        "                    text_feat = get_input_data(id_text[id1])\n",
        "                    batch_text_x.append(text_feat[0])\n",
        "                    batch_text2_x.append(text_feat[1])\n",
        "                    text_feat = get_input_data(id_text[id2])\n",
        "                    batch_text_x2.append(text_feat[0])\n",
        "                    batch_text2_x2.append(text_feat[1])                    \n",
        "                    y.append([id_label2.get(id1,-1),id_label2.get(id2,-1)])\n",
        "\n",
        "                except:\n",
        "                    print(id,str(traceback.format_exc()))\n",
        "            y = torch.FloatTensor(y).to('cuda')\n",
        "            batch_images_x1 = torch.FloatTensor(batch_images_x1).to('cuda')\n",
        "            batch_images_x2 = torch.FloatTensor(batch_images_x2).to('cuda')\n",
        "            batch_feat_x1 = torch.FloatTensor(batch_feat_x1).to('cuda')\n",
        "            batch_feat_x2 = torch.FloatTensor(batch_feat_x2).to('cuda')\n",
        "            batch_text_x = torch.stack(batch_text_x, 0).to('cuda')\n",
        "            batch_text2_x = torch.stack(batch_text2_x, 0).to('cuda')\n",
        "            batch_text_x2 = torch.stack(batch_text_x2, 0).to('cuda')\n",
        "            batch_text2_x2 = torch.stack(batch_text2_x2, 0).to('cuda')\n",
        "            output = model.pair_forward(x1 = [batch_images_x1.permute(0,3,1,2),batch_text_x,batch_text2_x,batch_feat_x1],\n",
        "                                    x2 = [batch_images_x2.permute(0,3,1,2),batch_text_x2,batch_text2_x2,batch_feat_x2])\n",
        "            output2 = F.sigmoid(output)\n",
        "            temp = F.sigmoid(output).tolist()\n",
        "            for i,id in enumerate(batch_ids):\n",
        "                id1 = id[0]\n",
        "                id2 = id[1]\n",
        "                id_pred2[id1] = id_pred2.get(id1,[])\n",
        "                id_pred2[id2] = id_pred2.get(id2,[])\n",
        "                id_pred2[id1].append(temp[i][0])\n",
        "                id_pred2[id2].append(temp[i][1])\n",
        "                \n",
        "        if MODE == \"VALID\":\n",
        "            valid_y2 = []    \n",
        "            pred_y2 = []\n",
        "            for id,prob in id_pred2.items():\n",
        "                if id_label2.get(id,-1) == -1:\n",
        "                    continue\n",
        "                pred_y2.append(sum(id_pred2[id])/len(id_pred2[id]))\n",
        "                valid_y2.append(id_label2[id])\n",
        "            print(sklearn.metrics.roc_auc_score(valid_y2,pred_y2), \n",
        "                  sklearn.metrics.accuracy_score(valid_y2,np.array(pred_y2)>0.5),\n",
        "                  sklearn.metrics.log_loss(valid_y2,np.array(pred_y2)))\n",
        "        \n",
        "    if not ISPREDICT:\n",
        "        torch.save(model.state_dict(), weight_path + '/model_1_' + str(fold) + '.pt')\n",
        "    id_pred2_all.append(id_pred2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42a4a04299434f319624e1de84bbd980",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ep 0\n",
            "loss: 0.024283420971211264 0.01734317978778306\n",
            "ep 1\n",
            "loss: 0.021965485791073127 0.016596747644245624\n",
            "ep 2\n",
            "loss: 0.020467764669919716 0.01286826821022174\n",
            "ep 3\n",
            "loss: 0.018541635106810752 0.010198480090674231\n",
            "ep 4\n",
            "loss: 0.01739519047627554 0.008821244323954862\n",
            "ep 5\n",
            "loss: 0.01577968000894522 0.007016938968616373\n",
            "ep 0\n",
            "loss: 0.023730824396014212 0.01701344163978801\n",
            "ep 1\n",
            "loss: 0.020660863497458836 0.013424133037819582\n",
            "ep 2\n",
            "loss: 0.019044325855286683 0.011263365357237704\n",
            "ep 3\n",
            "loss: 0.017848802167045718 0.009772705533925225\n",
            "ep 4\n",
            "loss: 0.016473738664925536 0.008053105056285858\n",
            "ep 5\n",
            "loss: 0.015744444661061553 0.006923178182805286\n",
            "ep 0\n",
            "loss: 0.024230350924765363 0.017332687707946583\n",
            "ep 1\n",
            "loss: 0.020482020119972088 0.01254472341476118\n",
            "ep 2\n",
            "loss: 0.018424879028626225 0.010219995540731093\n",
            "ep 3\n",
            "loss: 0.017225000129026526 0.008579644829473074\n",
            "ep 4\n",
            "loss: 0.015865829848848722 0.007041961715940167\n",
            "ep 5\n",
            "loss: 0.014829748408101938 0.0062666551405016115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLcXM-myeR2M"
      },
      "source": [
        "id_pred_2 = {}\n",
        "for id in ids_list3:\n",
        "    id_pred_2[id] = (id_pred_all[0][id] +  id_pred_all[1][id] + id_pred_all[2][id])/3\n",
        "            \n",
        "id_pred2_2 = {}\n",
        "for id,prob in id_pred2_all[0].items():\n",
        "    if id_label2.get(id,-1) == -1:\n",
        "        continue\n",
        "    id_pred2_2[id] = np.mean([np.mean(id_pred2_all[0][id]),np.mean(id_pred2_all[1][id]),np.mean(id_pred2_all[2][id])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u66OS1cfNNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01a4a93-c04d-4c51-feee-88de8116bfca"
      },
      "source": [
        "id_pred2_1_np = [[],[],[],[]]\n",
        "for id,prob in id_pred_all[0].items():\n",
        "    id_pred2_1_np[0].append(id_pred_all[0][id])\n",
        "    id_pred2_1_np[1].append(id_pred_all[1][id])\n",
        "    id_pred2_1_np[2].append(id_pred_all[2][id])\n",
        "    id_pred2_1_np[3].append(id_mmf.get(id,0))\n",
        "print(np.corrcoef(np.stack(id_pred2_1_np)))\n",
        "\n",
        "id_pred2_2_np = [[],[],[],[]]\n",
        "for id,prob in id_pred2_all[0].items():\n",
        "    if id_label2.get(id,-1) == -1:\n",
        "        continue\n",
        "    id_pred2_2_np[0].append(np.mean(id_pred2_all[0][id]))\n",
        "    id_pred2_2_np[1].append(np.mean(id_pred2_all[1][id]))\n",
        "    id_pred2_2_np[2].append(np.mean(id_pred2_all[2][id]))\n",
        "    id_pred2_2_np[3].append(id_mmf.get(id,0))\n",
        "print(np.corrcoef(np.stack(id_pred2_2_np)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.8469239  0.8700942  0.38117792]\n",
            " [0.8469239  1.         0.83566716 0.39374977]\n",
            " [0.8700942  0.83566716 1.         0.44103554]\n",
            " [0.38117792 0.39374977 0.44103554 1.        ]]\n",
            "[[1.         0.65417825 0.62479268 0.30998354]\n",
            " [0.65417825 1.         0.5752113  0.24736187]\n",
            " [0.62479268 0.5752113  1.         0.316159  ]\n",
            " [0.30998354 0.24736187 0.316159   1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03lv_Y33fXeM"
      },
      "source": [
        "import pickle\n",
        "f = open(\"/content/drive/MyDrive/hateful-memes/weight/submission_1.pkl\", 'wb')\n",
        "pickle.dump([id_pred_2,id_pred2_2,id_mmf], f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}